


    {.emit: ["""
    for (size_t in = 0; in < """, layer.numInputs, """; ++in)
        {
            auto& in_neurons_gradient_in = """, layerBackpropInfo.inputGradient, """.p->data[in];
#pragma omp simd reduction(+ : in_neurons_gradient_in)
            for (size_t out = 0; out < """, layer.numOutputs, """; ++out)
            {
                const size_t i = out + """, layer.numOutputs, """ * in;
                //weightsIndex(in, out, num_in_neurons, num_out_neurons);
                """, layerBackpropInfo.paramGradient.weights, """.p->data[i] +=
                    """, inPostActivation, """[in] * """, layerBackpropInfo.paramGradient.bias, """.p->data[out];
                in_neurons_gradient_in +=
                    """, layer.weights, """.p->data[i] * """, layerBackpropInfo.paramGradient.bias ,""".p->data[out];
            }
        }
    """].}




    # {.emit: ["""
    # auto& num_in_neurons = """, layer.numInputs, """;
    # auto& num_out_neurons = """, layer.numOutputs, """;
    # auto& out_neurons_activation = """, result, """.p->data;
    # auto& weights = """, layer.weights, """.p->data;
    # auto& in_neurons_activation = """, input, """;
    # for (size_t in = 0; in < num_in_neurons; ++in)
    # {
    #     for (size_t out = 0; out < num_out_neurons; ++out)
    #     {
    #         const size_t i = out + num_out_neurons * num_in_neurons; // TODO use weightsIndex
    #         out_neurons_activation[out] += weights[i] * in_neurons_activation[in];
    #     }
    # }
    # """].}





    {.emit: ["""
    auto& num_in_neurons = """, layer.numInputs, """;
    auto& num_out_neurons = """, layer.numOutputs, """;
    auto& in_neurons_gradient = """, layerBackpropInfo.inputGradient, """.p->data;
    auto& weights_gradient = """, layerBackpropInfo.paramGradient.weights, """.p->data;
    auto& in_neurons_activation = """, inPostActivation, """;
    auto& out_neurons_bias_gradient = """, layerBackpropInfo.paramGradient.bias, """.p->data;
    auto& weights = """, layer.weights, """.p->data;
    for (size_t in = 0; in < num_in_neurons; ++in)
    {
        float& in_neurons_gradient_in = in_neurons_gradient[in];
#pragma omp simd reduction(+ : in_neurons_gradient_in)
        for (size_t out = 0; out < num_out_neurons; ++out)
        {
            const size_t i = out + num_out_neurons * num_in_neurons; // TODO use weightsIndex
            weights_gradient[i] += in_neurons_activation[in] * out_neurons_bias_gradient[out];
            in_neurons_gradient_in += weights[i] * out_neurons_bias_gradient[out];
        }
    }
    """].}




{.emit: ["""
    for(size_t inNeuron = 0; inNeuron < """, layer.numInputs, """; ++inNeuron){
        auto& in_neurons_gradient_in = """, layerBackpropInfo.inputGradient, """.p->data[inNeuron];
        for(size_t outNeuron = 0; outNeuron < """, layer.numOutputs, """; ++outNeuron){
            const size_t i = outNeuron + """, layer.numOutputs, """ * inNeuron;
            """, layerBackpropInfo.paramGradient.weights, """.p->data[i] +=
                """, inPostActivation, """[inNeuron] * """, layerBackpropInfo.paramGradient.bias, """.p->data[outNeuron];
            in_neurons_gradient_in +=
                """, layer.weights, """.p->data[i] * """, layerBackpropInfo.paramGradient.bias, """.p->data[outNeuron];
        }
    }    
    """].}